# RAG Chatbot Package

RAG (Retrieval-Augmented Generation) chatbot system cho Q&A tá»« document summaries sá»­ dá»¥ng vector similarity search vÃ  Gemini AI.

## ğŸ¯ TÃ­nh nÄƒng chÃ­nh

- **Document Retrieval**: Vector search sá»­ dá»¥ng FAISS vÃ  sentence-transformers
- **Conversational AI**: Gemini API integration vá»›i conversation context
- **Local Embeddings**: Offline text embedding vá»›i all-MiniLM-L6-v2
- **REST API**: FastAPI server vá»›i comprehensive endpoints
- **Command-line Interface**: Interactive CLI cho development/testing
- **Document Management**: Chunking, metadata, filtering capabilities
- **Performance Optimization**: Index caching, multi-model fallback

## ğŸ“¦ Cáº¥u trÃºc Package

```
rag_chatbot/
â”œâ”€â”€ __init__.py                 # Package initialization
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ schemas.py                  # Pydantic data models
â”œâ”€â”€ embeddings.py               # Text embedding service
â”œâ”€â”€ vector_store.py            # FAISS vector database
â”œâ”€â”€ retriever.py               # Document retrieval system
â”œâ”€â”€ llm_adapter.py             # Gemini LLM integration
â”œâ”€â”€ chat_engine.py             # Core RAG chat engine
â”œâ”€â”€ main.py                    # Command-line interface
â”œâ”€â”€ api.py                     # FastAPI REST server
â”œâ”€â”€ demo.py                    # Example usage vÃ  demos
â”œâ”€â”€ README.md                  # Documentation
â”œâ”€â”€ data/
â”‚   â””â”€â”€ mock_summaries.json    # Sample document data
â””â”€â”€ cache/                     # FAISS index cache (auto-created)
```

## ğŸ› ï¸ CÃ i Ä‘áº·t

### 1. Dependencies

```bash
pip install -r requirements.txt
```

### 2. Environment Configuration

Táº¡o `.env` file á»Ÿ project root:

```env
# Gemini API Configuration
GEMINI_API_KEY=your_api_key_here
GEMINI_MODEL=gemini-2.0-flash-exp

# Development mode (sá»­ dá»¥ng canned responses)
USE_CANNED_LLM=false
```

### 3. Khá»Ÿi táº¡o System

```bash
# Initialize vÃ  build search index
python main.py --init

# Force rebuild index tá»« source data
python main.py --rebuild
```

## ğŸš€ Sá»­ dá»¥ng

### Command-line Interface

```bash
# Interactive chat mode
python main.py --interactive

# Single query
python main.py --query "Python cÃ³ nhá»¯ng Ä‘áº·c Ä‘iá»ƒm gÃ¬?"

# Custom data source
python main.py --data /path/to/your/summaries.json
```

### REST API Server

```bash
# Start API server (port 8006)
python api.py

# Custom host/port
python api.py --host localhost --port 8000 --reload
```

### Python Integration

```python
from rag_chatbot import get_chat_engine, RAGChatRequest, ChatConfig

# Initialize
chat_engine = get_chat_engine()
chat_engine.initialize()

# Create request
request = RAGChatRequest(
    query="Giáº£i thÃ­ch vá» Python programming language",
    chat_config=ChatConfig(temperature=0.7)
)

# Get response
response = chat_engine.chat(request)
print(response.answer)
```

## ğŸ“– API Endpoints

### Chat Endpoints

```http
POST /chat
Content-Type: application/json

{
  "query": "Python cÃ³ nhá»¯ng Æ°u Ä‘iá»ƒm gÃ¬?",
  "retrieval_config": {
    "top_k": 5,
    "similarity_threshold": 0.3
  },
  "chat_config": {
    "temperature": 0.7,
    "max_tokens": 2048,
    "include_sources": true
  }
}
```

### Quick Chat

```http
POST /chat/quick?query=Python+cÃ³+nhá»¯ng+Æ°u+Ä‘iá»ƒm+gÃ¬&top_k=3&temperature=0.8
```

### Document Search

```http
GET /search/documents?query=machine+learning&top_k=10&category=AI
```

### Conversation Management

```http
GET /conversations                          # List conversations
POST /conversations/{id}/chat              # Chat in conversation
GET /conversations/{id}                    # Get conversation
DELETE /conversations/{id}                 # Delete conversation
```

### Metadata

```http
GET /metadata/topics                       # List topics
GET /metadata/categories                   # List categories
GET /stats                                # System statistics
```

## ğŸ—ï¸ Kiáº¿n trÃºc System

### RAG Pipeline

1. **Query Processing**: Nháº­n user query vÃ  config
2. **Document Retrieval**: Vector search trong FAISS index
3. **Context Building**: Aggregate relevant document chunks
4. **LLM Generation**: Gemini API vá»›i augmented context
5. **Response Formatting**: Structure output vá»›i sources

### Core Components

- **TextEmbedding**: Sentence-transformers cho local embeddings
- **FAISSVectorStore**: Vector database vá»›i similarity search
- **DocumentRetriever**: Orchestrate search vÃ  ranking
- **RAGChatEngine**: Main conversation logic
- **GeminiChatAdapter**: LLM integration vá»›i retry logic

### Data Flow

```
User Query â†’ Embedding â†’ Vector Search â†’ Document Chunks â†’
Context Building â†’ Gemini LLM â†’ Generated Response
```

## âš™ï¸ Configuration

### Retrieval Config

```python
RetrievalConfig(
    top_k=5,                    # Number of documents to retrieve
    similarity_threshold=0.3,    # Minimum similarity score
    chunk_size=200,             # Characters per chunk
    chunk_overlap=50,           # Overlap between chunks
    topic_filter="Python",      # Filter by topic
    include_metadata=True       # Include document metadata
)
```

### Chat Config

```python
ChatConfig(
    temperature=0.7,            # LLM creativity (0.0-2.0)
    top_p=0.9,                 # Nucleus sampling
    max_tokens=2048,           # Max response length
    max_context_docs=5,        # Max docs in context
    include_sources=True,      # Include source references
    response_style="helpful"   # Response style preference
)
```

## ğŸ“Š Performance Features

### Caching Strategy

- **FAISS Index**: Persistent index cache Ä‘á»ƒ avoid rebuild
- **Embedding Cache**: Cache embeddings cho repeated queries
- **Model Selection**: Automatic fallback qua multiple Gemini models

### Optimization

- **Chunking**: Intelligent document splitting vá»›i overlap
- **Batch Processing**: Efficient embedding generation
- **Lazy Loading**: Initialize components on demand
- **Memory Management**: Optimized vector operations

## ğŸ§ª Testing & Demo

### Run Demo

```bash
# Comprehensive demo vá»›i sample queries
python demo.py
```

### Example Queries

```python
# Technical questions
"Python cÃ³ nhá»¯ng Ä‘áº·c Ä‘iá»ƒm gÃ¬ ná»•i báº­t?"
"So sÃ¡nh SQL vÃ  NoSQL databases"
"REST API best practices lÃ  gÃ¬?"

# Conversational follow-ups
"CÃ³ thá»ƒ sá»­ dá»¥ng Python Ä‘á»ƒ lÃ m gÃ¬?"
"Æ¯u Ä‘iá»ƒm cá»§a NoSQL lÃ  gÃ¬?"
"CÃ³ alternatives nÃ o cho REST khÃ´ng?"
```

## ğŸ”§ Development

### Adding New Documents

1. Update `data/mock_summaries.json`:

```json
{
  "id": "doc_new",
  "content": "Your document content...",
  "topic": "New Topic",
  "category": "New Category",
  "user_id": "user_id",
  "created_at": "2024-01-01T00:00:00Z",
  "tags": ["tag1", "tag2"]
}
```

2. Rebuild index:

```bash
python main.py --rebuild
```

### Custom Data Sources

```python
# Use custom JSON data
retriever = DocumentRetriever(data_path="/path/to/your/data.json")
chat_engine = RAGChatEngine(retriever=retriever)
```

### Extend LLM Integration

```python
# Custom LLM adapter
class CustomLLMAdapter:
    def generate_response(self, messages, config):
        # Your custom LLM logic
        pass

chat_engine = RAGChatEngine(llm_adapter=CustomLLMAdapter())
```

## ğŸš¨ Troubleshooting

### Common Issues

**Index Not Found**

```bash
# Rebuild FAISS index
python main.py --rebuild
```

**API Key Issues**

```bash
# Check .env file
echo $GEMINI_API_KEY

# Test with canned responses
USE_CANNED_LLM=true python main.py
```

**Memory Issues**

```python
# Reduce chunk size vÃ  batch size
RetrievalConfig(chunk_size=100, top_k=3)
```

**Performance Issues**

```bash
# Clear cache vÃ  rebuild
curl -X POST http://localhost:8006/admin/clear-cache
curl -X POST http://localhost:8006/admin/rebuild-index
```

### Debug Mode

```bash
# Enable verbose logging
python main.py --verbose

# Check system stats
curl http://localhost:8006/stats
```

## ğŸ”— Integration

### Vá»›i Quiz Generator

```python
# Use RAG for generating quiz questions
retriever = get_document_retriever()
docs = retriever.search_by_filters(category="Programming")
# Pass docs to quiz generator
```

### Vá»›i Other Packages

```python
# Share retriever instance
from rag_chatbot import get_document_retriever
from other_package import SomeService

retriever = get_document_retriever()
service = SomeService(knowledge_base=retriever)
```

## ğŸ“ˆ Monitoring

### API Monitoring

- Health check: `GET /health`
- System stats: `GET /stats`
- Performance metrics trong response times

### Logging

- Structured logging vá»›i timestamps
- Request/response tracking
- Error monitoring vÃ  alerting

## ğŸ›¡ï¸ Security

### API Security

- CORS configuration
- Input validation vá»›i Pydantic
- Rate limiting (cÃ³ thá»ƒ thÃªm middleware)
- Error handling khÃ´ng expose internals

### Data Privacy

- Local embedding models (no data sent to external services)
- Configurable data retention
- Conversation cleanup options

## ğŸ“š TÃ i liá»‡u bá»• sung

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [Sentence Transformers](https://www.sbert.net/)
- [Google Gemini API](https://ai.google.dev/)

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch
3. Add tests cho new functionality
4. Update documentation
5. Submit pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.
